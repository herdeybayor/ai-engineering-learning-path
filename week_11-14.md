# Working with Pre-trained Models - Weeks 11-14

## Introduction

Welcome to the next phase of your AI engineering journey! Over these four weeks, we'll dive into working with pre-trained models, focusing particularly on Large Language Models (LLMs). Think of pre-trained models as highly educated specialists that we can consult with and direct to perform specific tasks. We'll learn how to effectively communicate with these models and harness their capabilities for practical applications.

## Week 11: Getting Started with OpenAI API

### Day 1-2: Setting Up Your Development Environment

First, let's prepare your environment for working with the OpenAI API.

#### Environment Setup
```bash
# Activate your conda environment
conda activate aienv

# Install required packages
pip install openai python-dotenv requests
```

#### Creating a Basic Configuration
```python
# config.py
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure OpenAI
import openai
openai.api_key = os.getenv('OPENAI_API_KEY')
```

#### Setting Up Environment Variables
Create a `.env` file in your project root:
```plaintext
OPENAI_API_KEY=your_api_key_here
```

### Day 3-4: Making Your First API Calls

#### Basic Completion Example
```python
def get_completion(prompt, model="gpt-3.5-turbo"):
    """
    Get a completion from the OpenAI API.
    
    Args:
        prompt (str): The input prompt
        model (str): The model to use
        
    Returns:
        str: The model's response
    """
    try:
        messages = [{"role": "user", "content": prompt}]
        response = openai.ChatCompletion.create(
            model=model,
            messages=messages,
            temperature=0.7
        )
        return response.choices[0].message["content"]
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

# Example usage
prompt = "Explain what a pre-trained model is in simple terms."
response = get_completion(prompt)
print(response)
```

### Day 5: Understanding API Parameters

Let's explore the key parameters that control model behavior:

#### Temperature and Top-p
```python
def controlled_completion(prompt, temperature=0.7, top_p=1.0):
    """
    Get a completion with controlled randomness.
    
    Args:
        prompt (str): The input prompt
        temperature (float): Controls randomness (0-2)
        top_p (float): Controls diversity (0-1)
    """
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=temperature,
        top_p=top_p
    )
    return response.choices[0].message["content"]

# Example: Compare different temperatures
prompt = "Write a short story about a robot."
creative_story = controlled_completion(prompt, temperature=1.5)
factual_story = controlled_completion(prompt, temperature=0.2)
```

## Week 12: Advanced Prompt Engineering

### Day 1-2: Prompt Engineering Fundamentals

#### Basic Prompt Structure
```python
def structured_prompt(task, context, examples, question):
    """
    Create a structured prompt with context and examples.
    """
    prompt = f"""
    Task: {task}
    
    Context: {context}
    
    Examples:
    {examples}
    
    Question: {question}
    """
    return get_completion(prompt)

# Example usage
task = "Classify the sentiment of the given text"
context = "We're analyzing customer feedback for a product"
examples = """
Positive: "This product exceeded my expectations!"
Negative: "I'm disappointed with the quality"
"""
question = "Please analyze: 'The product works well but could be improved'"
```

### Day 3-4: Chain-of-Thought Prompting

#### Implementing Chain-of-Thought
```python
def chain_of_thought_prompt(problem):
    """
    Create a prompt that encourages step-by-step thinking.
    """
    prompt = f"""
    Problem: {problem}
    
    Let's solve this step by step:
    1) First, let's understand what we're being asked
    2) Then, let's identify the key information
    3) Next, let's plan our approach
    4) Finally, let's solve the problem
    
    Please show your work for each step.
    """
    return get_completion(prompt)
```

### Day 5: Role-Based Prompting

#### Creating Role-Based Interactions
```python
def expert_consultation(role, expertise, question):
    """
    Create a prompt that simulates consulting with an expert.
    """
    prompt = f"""
    You are a {role} with expertise in {expertise}.
    
    Background: {expertise}
    
    Question: {question}
    
    Please provide your expert analysis and recommendation.
    """
    return get_completion(prompt)
```

## Week 13: Working with Hugging Face

### Day 1-2: Setting Up Hugging Face

#### Environment Setup
```bash
pip install transformers torch datasets
```

#### Basic Model Loading
```python
from transformers import AutoTokenizer, AutoModel

def load_huggingface_model(model_name):
    """
    Load a pre-trained model and tokenizer from Hugging Face.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    return tokenizer, model

# Example usage
model_name = "bert-base-uncased"
tokenizer, model = load_huggingface_model(model_name)
```

### Day 3-4: Text Classification with Hugging Face

#### Creating a Text Classifier
```python
from transformers import pipeline

def setup_classifier():
    """
    Set up a text classification pipeline.
    """
    classifier = pipeline("sentiment-analysis")
    return classifier

def classify_text(classifier, text):
    """
    Classify the sentiment of input text.
    """
    result = classifier(text)
    return result[0]

# Example usage
classifier = setup_classifier()
result = classify_text(classifier, "I love working with AI!")
```

### Day 5: Fine-tuning Models

#### Basic Fine-tuning Setup
```python
from transformers import Trainer, TrainingArguments

def prepare_fine_tuning(model, train_dataset, eval_dataset):
    """
    Prepare a model for fine-tuning.
    """
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=3,
        per_device_train_batch_size=16,
        evaluation_strategy="epoch"
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset
    )
    return trainer
```

## Week 14: Building Practical Applications

### Day 1-2: Creating a Question-Answering System

#### Building a QA System
```python
def create_qa_system():
    """
    Create a simple question-answering system.
    """
    from transformers import pipeline
    
    qa_pipeline = pipeline("question-answering")
    return qa_pipeline

def answer_question(qa_pipeline, context, question):
    """
    Get an answer to a question based on the given context.
    """
    result = qa_pipeline(context=context, question=question)
    return result['answer']
```

### Day 3-4: Text Generation System

#### Implementing Text Generation
```python
def generate_text(prompt, max_length=100):
    """
    Generate text based on a prompt.
    """
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=max_length
    )
    return response.choices[0].message["content"]

def create_content_generator(template):
    """
    Create a specialized content generator.
    """
    def generate_content(**kwargs):
        formatted_prompt = template.format(**kwargs)
        return generate_text(formatted_prompt)
    return generate_content
```

### Day 5: Token Management and Cost Optimization

#### Token Counter Implementation
```python
def count_tokens(text, model="gpt-3.5-turbo"):
    """
    Count the number of tokens in a text.
    """
    from transformers import GPT2Tokenizer
    
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokens = tokenizer.encode(text)
    return len(tokens)

def estimate_cost(tokens, model="gpt-3.5-turbo"):
    """
    Estimate the cost of an API call based on tokens.
    """
    rates = {
        "gpt-3.5-turbo": 0.002 / 1000,  # $0.002 per 1k tokens
        "gpt-4": 0.03 / 1000,           # $0.03 per 1k tokens
    }
    return tokens * rates.get(model, 0)
```

## Practice Projects

1. Create a sentiment analysis bot for product reviews
2. Build a content summarization system
3. Develop a language translation service
4. Implement a chatbot with personality
5. Create a text-to-SQL query generator

## Best Practices and Tips

1. Always implement error handling for API calls
2. Monitor and log token usage
3. Use appropriate temperature settings for different tasks
4. Keep prompts clear and well-structured
5. Implement rate limiting for API calls
6. Cache responses when appropriate
7. Test with various inputs and edge cases

## Additional Resources

1. OpenAI API Documentation
2. Hugging Face Documentation
3. Prompt Engineering Guide by OpenAI
4. Model Cards for various pre-trained models
5. AI Safety and Ethics Guidelines

Remember: Working with pre-trained models is as much an art as it is a science. Practice different prompting techniques and pay attention to how small changes in your prompts can lead to significant differences in results. Happy coding!
